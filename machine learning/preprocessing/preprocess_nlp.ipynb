{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGEjtmTND83z",
        "outputId": "b67d1f1c-0052-497f-ca35-e0e7405a3429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n",
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bXL9YSdMDyws",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpU1McvZDywu",
        "outputId": "c5aea8cb-2fbe-4a7f-c085-ca081a30434d",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Download required NLTK data (run this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8T13z-Q1Dywv",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sample text for demonstration\n",
        "text = \"\"\"Hey there! This is an example text with some numbers (123) and special characters @#$.\n",
        "         I love programming in Python! Python is amazing... Check out https://www.example.com\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6EKIghf6Dywv",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Step 1: Convert to lowercase\n",
        "    # This helps in standardizing the text\n",
        "    text = text.lower()\n",
        "    print(\"\\n--> After lowercase:\", text)\n",
        "\n",
        "    # Step 2: Remove URLs\n",
        "    # We use regex to find and remove web links\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    print(\"\\n--> After removing URLs:\", text)\n",
        "\n",
        "    # Step 3: Remove special characters and numbers\n",
        "    # Keep only letters and spaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    print(\"\\n--> After removing special characters:\", text)\n",
        "\n",
        "    # Step 4: Tokenization\n",
        "    # Split text into individual words\n",
        "    tokens = word_tokenize(text)\n",
        "    print(\"\\n--> After tokenization:\", tokens)\n",
        "\n",
        "    # Step 5: Remove stopwords\n",
        "    # Stopwords are common words like 'the', 'is', 'at' that don't carry much meaning\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    print(\"\\n--> After removing stopwords:\", tokens)\n",
        "\n",
        "    # Step 6: Lemmatization\n",
        "    # Convert words to their base form (e.g., 'running' -> 'run')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    print(\"\\n--> After lemmatization:\", tokens)\n",
        "\n",
        "    # Step 7: Stemming (optional)\n",
        "    # Another way to get root form of words, but can be more aggressive\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    print(\"\\n--> After stemming:\", tokens)\n",
        "\n",
        "    # Step 8: Join tokens back to text\n",
        "    clean_text = ' '.join(tokens)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4miiutEfDyww",
        "outputId": "316bee4f-17ef-46e0-c51a-74759f23dd64",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--> After lowercase: hey there! this is an example text with some numbers (123) and special characters @#$. \n",
            "         i love programming in python! python is amazing... check out https://www.example.com\n",
            "\n",
            "--> After removing URLs: hey there! this is an example text with some numbers (123) and special characters @#$. \n",
            "         i love programming in python! python is amazing... check out \n",
            "\n",
            "--> After removing special characters: hey there this is an example text with some numbers  and special characters  \n",
            "         i love programming in python python is amazing check out \n",
            "\n",
            "--> After tokenization: ['hey', 'there', 'this', 'is', 'an', 'example', 'text', 'with', 'some', 'numbers', 'and', 'special', 'characters', 'i', 'love', 'programming', 'in', 'python', 'python', 'is', 'amazing', 'check', 'out']\n",
            "\n",
            "--> After removing stopwords: ['hey', 'example', 'text', 'numbers', 'special', 'characters', 'love', 'programming', 'python', 'python', 'amazing', 'check']\n",
            "\n",
            "--> After lemmatization: ['hey', 'example', 'text', 'number', 'special', 'character', 'love', 'programming', 'python', 'python', 'amazing', 'check']\n",
            "\n",
            "--> After stemming: ['hey', 'exampl', 'text', 'number', 'special', 'charact', 'love', 'program', 'python', 'python', 'amaz', 'check']\n",
            "\n",
            "--> Final cleaned text: hey exampl text number special charact love program python python amaz check\n"
          ]
        }
      ],
      "source": [
        "# Process the text and show final result\n",
        "final_text = clean_text(text)\n",
        "print(\"\\n--> Final cleaned text:\", final_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_stsV0c8Dyww",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Bonus: Example with multiple texts using pandas\n",
        "def process_multiple_texts():\n",
        "    # Create a sample DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'text': [\n",
        "            \"First example! With numbers 123\",\n",
        "            \"Second example... with special chars @#$\",\n",
        "            \"Third example with URL https://example.com\"\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    # Apply cleaning function to entire column\n",
        "    df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "    print(\"\\nProcessing multiple texts using pandas:\")\n",
        "    print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIpCjiR-Dywx",
        "outputId": "37575c26-5f3f-4d1f-846c-627fa9afd28f",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--> After lowercase: first example! with numbers 123\n",
            "\n",
            "--> After removing URLs: first example! with numbers 123\n",
            "\n",
            "--> After removing special characters: first example with numbers \n",
            "\n",
            "--> After tokenization: ['first', 'example', 'with', 'numbers']\n",
            "\n",
            "--> After removing stopwords: ['first', 'example', 'numbers']\n",
            "\n",
            "--> After lemmatization: ['first', 'example', 'number']\n",
            "\n",
            "--> After stemming: ['first', 'exampl', 'number']\n",
            "\n",
            "--> After lowercase: second example... with special chars @#$\n",
            "\n",
            "--> After removing URLs: second example... with special chars @#$\n",
            "\n",
            "--> After removing special characters: second example with special chars \n",
            "\n",
            "--> After tokenization: ['second', 'example', 'with', 'special', 'chars']\n",
            "\n",
            "--> After removing stopwords: ['second', 'example', 'special', 'chars']\n",
            "\n",
            "--> After lemmatization: ['second', 'example', 'special', 'char']\n",
            "\n",
            "--> After stemming: ['second', 'exampl', 'special', 'char']\n",
            "\n",
            "--> After lowercase: third example with url https://example.com\n",
            "\n",
            "--> After removing URLs: third example with url \n",
            "\n",
            "--> After removing special characters: third example with url \n",
            "\n",
            "--> After tokenization: ['third', 'example', 'with', 'url']\n",
            "\n",
            "--> After removing stopwords: ['third', 'example', 'url']\n",
            "\n",
            "--> After lemmatization: ['third', 'example', 'url']\n",
            "\n",
            "--> After stemming: ['third', 'exampl', 'url']\n",
            "\n",
            "Processing multiple texts using pandas:\n",
            "                                         text                cleaned_text\n",
            "0             First example! With numbers 123         first exampl number\n",
            "1    Second example... with special chars @#$  second exampl special char\n",
            "2  Third example with URL https://example.com            third exampl url\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to see multiple text processing example\n",
        "# process_multiple_texts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fj2aLqHTEP77"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
