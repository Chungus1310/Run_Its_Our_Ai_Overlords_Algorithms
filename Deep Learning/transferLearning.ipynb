{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision.datasets import OxfordIIITPet"
      ],
      "metadata": {
        "id": "M8K4pojbJkd8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTL4GhIDJla4",
        "outputId": "08d2c637-f57d-460e-d744-bd07144f945e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define data transforms\n",
        "# We need to match MobileNet's expected input size (224x224) and normalization\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "zCfX5nq0Jm4I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and transform the dataset\n",
        "# Replace the dataset loading section with Oxford-IIIT Pet Dataset\n",
        "print(\"Downloading and preparing the Oxford-IIIT Pet Dataset...\")\n",
        "dataset = OxfordIIITPet(\n",
        "    root='./data',\n",
        "    split='trainval',\n",
        "    download=True,\n",
        "    transform=data_transforms\n",
        ")\n",
        "\n",
        "# Split dataset into train and test\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZaE1Tz9JobH",
        "outputId": "a9518ca3-b7c6-4b38-a86d-10d1a30ec02c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing the Oxford-IIIT Pet Dataset...\n",
            "Downloading https://thor.robots.ox.ac.uk/pets/images.tar.gz to data/oxford-iiit-pet/images.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 792M/792M [00:30<00:00, 25.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/oxford-iiit-pet/images.tar.gz to data/oxford-iiit-pet\n",
            "Downloading https://thor.robots.ox.ac.uk/pets/annotations.tar.gz to data/oxford-iiit-pet/annotations.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19.2M/19.2M [00:01<00:00, 10.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/oxford-iiit-pet/annotations.tar.gz to data/oxford-iiit-pet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Load pre-trained MobileNet model\n",
        "# We're using MobileNetV2 which is smaller and faster than many other models\n",
        "model = torchvision.models.mobilenet_v2(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPypihp-Jp6i",
        "outputId": "d9374e07-18a5-4c51-9067-72aaadf164ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 128MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Freeze the feature extraction layers\n",
        "# This prevents the pre-trained weights from being updated during initial training\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "mKq1yQNpJq-r"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Modify the classifier for our classification task\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 37)  # 37 classes in Oxford-IIIT\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "UYnNVNo9Jr_D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# We only optimize the classifier parameters to speed up training\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "1volUIKGJtbV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Training function\n",
        "def train_model(model, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        accuracy = 100. * correct / total\n",
        "        print(f'Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "N9o54c68Jvko"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Testing function\n",
        "def test_model(model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "    return accuracy\n",
        "\n",
        "# Add new functions for prediction\n",
        "def load_and_preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess a single image for prediction\"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    return data_transforms(image).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def get_class_name(idx):\n",
        "    \"\"\"Convert class index to pet breed name\"\"\"\n",
        "    class_names = dataset.classes\n",
        "    return class_names[idx]\n",
        "\n",
        "def predict_image(model, image_path):\n",
        "    \"\"\"Predict the class of a single image\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        image = load_and_preprocess_image(image_path).to(device)\n",
        "        outputs = model(image)\n",
        "        _, predicted = outputs.max(1)\n",
        "        return predicted.item(), get_class_name(predicted.item())\n",
        "\n",
        "def load_saved_model():\n",
        "    \"\"\"Load the saved model\"\"\"\n",
        "    model = torchvision.models.mobilenet_v2(pretrained=False)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, 37)\n",
        "    model.load_state_dict(torch.load('cat_dog_classifier.pth'))\n",
        "    model = model.to(device)\n",
        "    return model"
      ],
      "metadata": {
        "id": "exgv-1DuJyI6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Train and test the model\n",
        "if __name__ == \"__main__\":\n",
        "    # Training section\n",
        "    print(\"Starting training...\")\n",
        "    train_model(model)\n",
        "    print(\"\\nTesting the model...\")\n",
        "    test_accuracy = test_model(model)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), 'cat_dog_classifier.pth')\n",
        "    print(\"\\nModel saved successfully!\")\n",
        "\n",
        "    # Prediction section\n",
        "    print(\"\\nPrediction Demo:\")\n",
        "    # Load the saved model\n",
        "    loaded_model = load_saved_model()\n",
        "    loaded_model.eval()\n",
        "\n",
        "    # Example of predicting images from a test directory\n",
        "    test_dir = '/content/test_images'  # Create this directory and put some test images in it\n",
        "    if os.path.exists(test_dir):\n",
        "        for image_file in os.listdir(test_dir):\n",
        "            if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_path = os.path.join(test_dir, image_file)\n",
        "                _, breed_name = predict_image(loaded_model, image_path)\n",
        "                print(f\"Image: {image_file} -> Predicted: {breed_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piZxavRnIRYy",
        "outputId": "308ea476-00e2-4f80-a419-dff880dddca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1: Loss = 1.7612, Accuracy = 59.68%\n",
            "Epoch 2: Loss = 0.6381, Accuracy = 86.48%\n",
            "Epoch 3: Loss = 0.4357, Accuracy = 89.50%\n",
            "Epoch 4: Loss = 0.3411, Accuracy = 92.05%\n",
            "Epoch 5: Loss = 0.2902, Accuracy = 93.48%\n",
            "Epoch 6: Loss = 0.2422, Accuracy = 94.36%\n",
            "Epoch 7: Loss = 0.2096, Accuracy = 95.31%\n",
            "Epoch 8: Loss = 0.1882, Accuracy = 95.89%\n",
            "Epoch 9: Loss = 0.1757, Accuracy = 95.89%\n",
            "Epoch 10: Loss = 0.1645, Accuracy = 96.30%\n",
            "\n",
            "Testing the model...\n",
            "Test Accuracy: 89.27%\n",
            "\n",
            "Model saved successfully!\n",
            "\n",
            "Prediction Demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-10-32e768e82b49>:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('cat_dog_classifier.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: 3.jpg -> Predicted: Egyptian Mau\n",
            "Image: 1.jpg -> Predicted: Bengal\n",
            "Image: 2.jpeg -> Predicted: Egyptian Mau\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}