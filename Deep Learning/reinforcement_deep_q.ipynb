{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "from collections import deque\n",
        "import random"
      ],
      "metadata": {
        "id": "2kikEMYPP84A"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our neural network architecture for DQN\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    Deep Q-Network that takes the state as input and outputs Q-values for each action.\n",
        "    In CartPole, we have 4 state values and 2 possible actions (left/right).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        # Create a simple feedforward neural network\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),    # First hidden layer with 64 neurons\n",
        "            nn.ReLU(),                    # Activation function\n",
        "            nn.Linear(64, 64),            # Second hidden layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, output_size)    # Output layer (Q-values for each action)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ],
      "metadata": {
        "id": "_q1wll35P-ON"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Replay Memory stores past experiences for training.\n",
        "    This helps break correlation between consecutive samples and stabilizes learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque(maxlen=capacity)  # Using deque with fixed max length\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # Store transition (state, action, reward, next_state, done) in memory\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Randomly sample a batch of transitions from memory\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "cgeYgcbKP_Yf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \"\"\"\n",
        "    DQN Agent that interacts with and learns from the environment\n",
        "    \"\"\"\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99          # Discount factor for future rewards\n",
        "        self.epsilon = 1.0         # Exploration rate\n",
        "        self.epsilon_min = 0.01    # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.995 # Decay rate for exploration\n",
        "        self.learning_rate = 0.001 # Learning rate for the optimizer\n",
        "        self.batch_size = 64       # Size of batch to sample from replay memory\n",
        "\n",
        "        # Create main network and target network (for stable learning)\n",
        "        self.policy_net = DQN(state_size, action_size)\n",
        "        self.target_net = DQN(state_size, action_size)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        # Setup optimizer\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Initialize replay memory\n",
        "        self.memory = ReplayMemory(10000)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        Select action using epsilon-greedy policy:\n",
        "        - With probability epsilon: select random action (explore)\n",
        "        - Otherwise: select best action according to policy network (exploit)\n",
        "        \"\"\"\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.policy_net(state)\n",
        "            return q_values.max(1)[1].item()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Train the network using a batch of experiences from replay memory\n",
        "        \"\"\"\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample random batch from memory\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "\n",
        "        # Convert batch-array of transitions to transition of batch-arrays\n",
        "        batch = list(zip(*transitions))\n",
        "\n",
        "        # Extract each component of the transition\n",
        "        states = torch.FloatTensor(batch[0])\n",
        "        actions = torch.LongTensor(batch[1]).unsqueeze(1)\n",
        "        rewards = torch.FloatTensor(batch[2]).unsqueeze(1)\n",
        "        next_states = torch.FloatTensor(batch[3])\n",
        "        dones = torch.FloatTensor(batch[4]).unsqueeze(1)\n",
        "\n",
        "        # Compute current Q values\n",
        "        current_q_values = self.policy_net(states).gather(1, actions)\n",
        "\n",
        "        # Compute next Q values using target network\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # Compute expected Q values using Bellman equation\n",
        "        expected_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
        "\n",
        "        # Compute loss (Mean Squared Error between current and expected Q values)\n",
        "        loss = nn.MSELoss()(current_q_values, expected_q_values)\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Decay epsilon for less exploration over time\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"Update target network by copying parameters from policy network\"\"\"\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n"
      ],
      "metadata": {
        "id": "lgWuGF_JQB4t"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent():\n",
        "    \"\"\"\n",
        "    Main training loop\n",
        "    \"\"\"\n",
        "    # Create CartPole environment\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    # Initialize agent\n",
        "    state_size = env.observation_space.shape[0]  # 4 for CartPole\n",
        "    action_size = env.action_space.n             # 2 for CartPole\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    # Training parameters\n",
        "    n_episodes = 500\n",
        "    max_steps = 500\n",
        "    target_update = 10  # Update target network every 10 episodes\n",
        "\n",
        "    # Lists to store rewards for plotting\n",
        "    episode_rewards = []\n",
        "\n",
        "    # Training loop\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        # Episode loop\n",
        "        for step in range(max_steps):\n",
        "            # Select and perform action\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Store transition in memory\n",
        "            agent.memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "            # Train the network\n",
        "            agent.train()\n",
        "\n",
        "            # Update state and reward\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode % target_update == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        # Print progress\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            avg_reward = np.mean(episode_rewards[-10:])\n",
        "            print(f\"Episode {episode + 1}/{n_episodes}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    env.close()\n",
        "    return episode_rewards\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    rewards = train_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THGV9YMAPS1m",
        "outputId": "f63015f4-956d-4984-83e4-4d3036e13453"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10/500, Average Reward: 21.30\n",
            "Episode 20/500, Average Reward: 13.10\n",
            "Episode 30/500, Average Reward: 11.70\n",
            "Episode 40/500, Average Reward: 12.70\n",
            "Episode 50/500, Average Reward: 66.80\n",
            "Episode 60/500, Average Reward: 149.10\n",
            "Episode 70/500, Average Reward: 171.80\n",
            "Episode 80/500, Average Reward: 169.70\n",
            "Episode 90/500, Average Reward: 173.80\n",
            "Episode 100/500, Average Reward: 187.70\n",
            "Episode 110/500, Average Reward: 167.50\n",
            "Episode 120/500, Average Reward: 173.30\n",
            "Episode 130/500, Average Reward: 188.80\n",
            "Episode 140/500, Average Reward: 162.00\n",
            "Episode 150/500, Average Reward: 174.40\n",
            "Episode 160/500, Average Reward: 191.50\n",
            "Episode 170/500, Average Reward: 269.90\n",
            "Episode 180/500, Average Reward: 242.40\n",
            "Episode 190/500, Average Reward: 206.20\n",
            "Episode 200/500, Average Reward: 301.30\n",
            "Episode 210/500, Average Reward: 138.60\n",
            "Episode 220/500, Average Reward: 250.70\n",
            "Episode 230/500, Average Reward: 253.70\n",
            "Episode 240/500, Average Reward: 228.00\n",
            "Episode 250/500, Average Reward: 207.30\n",
            "Episode 260/500, Average Reward: 171.80\n",
            "Episode 270/500, Average Reward: 246.60\n",
            "Episode 280/500, Average Reward: 299.40\n",
            "Episode 290/500, Average Reward: 229.30\n",
            "Episode 300/500, Average Reward: 251.90\n",
            "Episode 310/500, Average Reward: 322.80\n",
            "Episode 320/500, Average Reward: 350.40\n",
            "Episode 330/500, Average Reward: 288.90\n",
            "Episode 340/500, Average Reward: 272.20\n",
            "Episode 350/500, Average Reward: 163.30\n",
            "Episode 360/500, Average Reward: 152.10\n",
            "Episode 370/500, Average Reward: 145.10\n",
            "Episode 380/500, Average Reward: 131.80\n",
            "Episode 390/500, Average Reward: 143.60\n",
            "Episode 400/500, Average Reward: 140.00\n",
            "Episode 410/500, Average Reward: 139.90\n",
            "Episode 420/500, Average Reward: 149.50\n",
            "Episode 430/500, Average Reward: 163.40\n",
            "Episode 440/500, Average Reward: 181.10\n",
            "Episode 450/500, Average Reward: 166.20\n",
            "Episode 460/500, Average Reward: 169.30\n",
            "Episode 470/500, Average Reward: 159.70\n",
            "Episode 480/500, Average Reward: 175.30\n",
            "Episode 490/500, Average Reward: 165.60\n",
            "Episode 500/500, Average Reward: 173.60\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}