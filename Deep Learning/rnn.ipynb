{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "VIB6ZRl82jNi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download and prepare Shakespeare data\n",
        "def get_shakespeare_data():\n",
        "    \"\"\"Download a sample of Shakespeare text\"\"\"\n",
        "    url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    text = requests.get(url).text\n",
        "    return text"
      ],
      "metadata": {
        "id": "0FVGpMBi2kOf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a character-level tokenizer\n",
        "class TextTokenizer:\n",
        "    def __init__(self, text):\n",
        "        # Create a mapping from characters to integers and vice versa\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "    def encode(self, string):\n",
        "        \"\"\"Convert string to list of integers\"\"\"\n",
        "        return [self.char_to_idx[ch] for ch in string]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert list of integers back to string\"\"\"\n",
        "        return ''.join([self.idx_to_char[idx] for idx in indices])"
      ],
      "metadata": {
        "id": "Bn2FsG8n2l7R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create a Dataset class\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, text, sequence_length=100):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.text = text\n",
        "        self.tokenizer = TextTokenizer(text)\n",
        "        self.encoded = self.tokenizer.encode(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get input sequence and target (next character)\n",
        "        sequence = self.encoded[idx:idx + self.sequence_length]\n",
        "        target = self.encoded[idx + 1:idx + self.sequence_length + 1]\n",
        "        return torch.LongTensor(sequence), torch.LongTensor(target)"
      ],
      "metadata": {
        "id": "s0yBYL1M2omg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the RNN model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=256, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        # Embedding layer converts character indices to dense vectors\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # RNN layer processes the sequence\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        # Linear layer for output\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        # Convert indices to embeddings\n",
        "        embedded = self.embedding(x)\n",
        "        # Process through RNN\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        # Convert RNN output to character probabilities\n",
        "        output = self.fc(output)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "J3RyTk4O2p74"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training function\n",
        "def train_model(model, dataset, epochs=10, batch_size=64, learning_rate=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Create DataLoader for batch processing\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, _ = model(inputs)\n",
        "            loss = criterion(outputs.view(-1, dataset.tokenizer.vocab_size), targets.view(-1))\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        print(f'Epoch {epoch+1} completed, Average Loss: {total_loss/len(dataloader):.4f}')"
      ],
      "metadata": {
        "id": "ns_PL2St2sEb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Text generation function\n",
        "def generate_text(model, tokenizer, seed_text, length=200, temperature=0.8):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    current_text = seed_text\n",
        "    generated_text = seed_text\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # Prepare input\n",
        "            sequence = tokenizer.encode(current_text)\n",
        "            sequence = torch.LongTensor([sequence]).to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            output, _ = model(sequence)\n",
        "\n",
        "            # Apply temperature to adjust randomness\n",
        "            probs = (output[0, -1] / temperature).softmax(dim=-1)\n",
        "            next_char_idx = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Append generated character\n",
        "            next_char = tokenizer.decode([next_char_idx])\n",
        "            generated_text += next_char\n",
        "            current_text = current_text[1:] + next_char\n",
        "\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "NziW2uog2t_6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Main execution\n",
        "def main():\n",
        "    # Get data\n",
        "    print(\"Downloading Shakespeare text...\")\n",
        "    text = get_shakespeare_data()\n",
        "\n",
        "    # Create dataset\n",
        "    print(\"Creating dataset...\")\n",
        "    dataset = ShakespeareDataset(text)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"Initializing model...\")\n",
        "    model = SimpleRNN(dataset.tokenizer.vocab_size)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    train_model(model, dataset, epochs=5)\n",
        "\n",
        "    # Generate text\n",
        "    print(\"\\nGenerating text...\")\n",
        "    seed_text = \"To be or not to be\"\n",
        "    generated = generate_text(model, dataset.tokenizer, seed_text)\n",
        "    print(f\"\\nGenerated text:\\n{generated}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-ulPbPr2Y2d",
        "outputId": "e68141d7-2b92-45db-9cf7-7b51eed243ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Shakespeare text...\n",
            "Creating dataset...\n",
            "Initializing model...\n",
            "Training model...\n",
            "Epoch 1, Batch 0, Loss: 4.2126\n",
            "Epoch 1, Batch 100, Loss: 2.0825\n",
            "Epoch 1, Batch 200, Loss: 1.8989\n",
            "Epoch 1, Batch 300, Loss: 1.7842\n",
            "Epoch 1, Batch 400, Loss: 1.7019\n",
            "Epoch 1, Batch 500, Loss: 1.6748\n",
            "Epoch 1, Batch 600, Loss: 1.6143\n",
            "Epoch 1, Batch 700, Loss: 1.6090\n",
            "Epoch 1, Batch 800, Loss: 1.6027\n",
            "Epoch 1, Batch 900, Loss: 1.5451\n",
            "Epoch 1, Batch 1000, Loss: 1.5328\n",
            "Epoch 1, Batch 1100, Loss: 1.5413\n",
            "Epoch 1, Batch 1200, Loss: 1.5318\n",
            "Epoch 1, Batch 1300, Loss: 1.4999\n",
            "Epoch 1, Batch 1400, Loss: 1.5014\n",
            "Epoch 1, Batch 1500, Loss: 1.4462\n",
            "Epoch 1, Batch 1600, Loss: 1.4818\n",
            "Epoch 1, Batch 1700, Loss: 1.4752\n",
            "Epoch 1, Batch 1800, Loss: 1.4756\n",
            "Epoch 1, Batch 1900, Loss: 1.4748\n",
            "Epoch 1, Batch 2000, Loss: 1.4046\n",
            "Epoch 1, Batch 2100, Loss: 1.4100\n",
            "Epoch 1, Batch 2200, Loss: 1.4579\n",
            "Epoch 1, Batch 2300, Loss: 1.4027\n",
            "Epoch 1, Batch 2400, Loss: 1.4286\n",
            "Epoch 1, Batch 2500, Loss: 1.4242\n",
            "Epoch 1, Batch 2600, Loss: 1.4664\n",
            "Epoch 1, Batch 2700, Loss: 1.4349\n",
            "Epoch 1, Batch 2800, Loss: 1.4213\n",
            "Epoch 1, Batch 2900, Loss: 1.4269\n",
            "Epoch 1, Batch 3000, Loss: 1.4481\n",
            "Epoch 1, Batch 3100, Loss: 1.4085\n",
            "Epoch 1, Batch 3200, Loss: 1.3716\n",
            "Epoch 1, Batch 3300, Loss: 1.3989\n",
            "Epoch 1, Batch 3400, Loss: 1.4066\n",
            "Epoch 1, Batch 3500, Loss: 1.4155\n",
            "Epoch 1, Batch 3600, Loss: 1.3709\n",
            "Epoch 1, Batch 3700, Loss: 1.4196\n",
            "Epoch 1, Batch 3800, Loss: 1.4243\n",
            "Epoch 1, Batch 3900, Loss: 1.3654\n",
            "Epoch 1, Batch 4000, Loss: 1.3660\n",
            "Epoch 1, Batch 4100, Loss: 1.4209\n",
            "Epoch 1, Batch 4200, Loss: 1.3734\n",
            "Epoch 1, Batch 4300, Loss: 1.3630\n",
            "Epoch 1, Batch 4400, Loss: 1.4079\n",
            "Epoch 1, Batch 4500, Loss: 1.3879\n",
            "Epoch 1, Batch 4600, Loss: 1.3474\n",
            "Epoch 1, Batch 4700, Loss: 1.3810\n",
            "Epoch 1, Batch 4800, Loss: 1.3686\n",
            "Epoch 1, Batch 4900, Loss: 1.3901\n",
            "Epoch 1, Batch 5000, Loss: 1.4076\n",
            "Epoch 1, Batch 5100, Loss: 1.4152\n",
            "Epoch 1, Batch 5200, Loss: 1.3810\n",
            "Epoch 1, Batch 5300, Loss: 1.3464\n",
            "Epoch 1, Batch 5400, Loss: 1.3713\n",
            "Epoch 1, Batch 5500, Loss: 1.3985\n",
            "Epoch 1, Batch 5600, Loss: 1.3675\n",
            "Epoch 1, Batch 5700, Loss: 1.3805\n",
            "Epoch 1, Batch 5800, Loss: 1.3196\n",
            "Epoch 1, Batch 5900, Loss: 1.3353\n",
            "Epoch 1, Batch 6000, Loss: 1.3472\n",
            "Epoch 1, Batch 6100, Loss: 1.3633\n",
            "Epoch 1, Batch 6200, Loss: 1.3888\n",
            "Epoch 1, Batch 6300, Loss: 1.3587\n",
            "Epoch 1, Batch 6400, Loss: 1.4145\n",
            "Epoch 1, Batch 6500, Loss: 1.3640\n",
            "Epoch 1, Batch 6600, Loss: 1.3575\n",
            "Epoch 1, Batch 6700, Loss: 1.3298\n",
            "Epoch 1, Batch 6800, Loss: 1.3736\n",
            "Epoch 1, Batch 6900, Loss: 1.3684\n",
            "Epoch 1, Batch 7000, Loss: 1.3141\n",
            "Epoch 1, Batch 7100, Loss: 1.3153\n",
            "Epoch 1, Batch 7200, Loss: 1.3455\n",
            "Epoch 1, Batch 7300, Loss: 1.3124\n",
            "Epoch 1, Batch 7400, Loss: 1.3547\n",
            "Epoch 1, Batch 7500, Loss: 1.3685\n",
            "Epoch 1, Batch 7600, Loss: 1.3510\n",
            "Epoch 1, Batch 7700, Loss: 1.3165\n",
            "Epoch 1, Batch 7800, Loss: 1.3619\n",
            "Epoch 1, Batch 7900, Loss: 1.3755\n",
            "Epoch 1, Batch 8000, Loss: 1.3564\n",
            "Epoch 1, Batch 8100, Loss: 1.3345\n",
            "Epoch 1, Batch 8200, Loss: 1.3442\n",
            "Epoch 1, Batch 8300, Loss: 1.3602\n",
            "Epoch 1, Batch 8400, Loss: 1.3407\n",
            "Epoch 1, Batch 8500, Loss: 1.3671\n",
            "Epoch 1, Batch 8600, Loss: 1.4016\n",
            "Epoch 1, Batch 8700, Loss: 1.3337\n",
            "Epoch 1, Batch 8800, Loss: 1.3663\n",
            "Epoch 1, Batch 8900, Loss: 1.3562\n",
            "Epoch 1, Batch 9000, Loss: 1.3586\n",
            "Epoch 1, Batch 9100, Loss: 1.3249\n",
            "Epoch 1, Batch 9200, Loss: 1.3313\n",
            "Epoch 1, Batch 9300, Loss: 1.3475\n",
            "Epoch 1, Batch 9400, Loss: 1.3474\n",
            "Epoch 1, Batch 9500, Loss: 1.3469\n",
            "Epoch 1, Batch 9600, Loss: 1.3830\n",
            "Epoch 1, Batch 9700, Loss: 1.3285\n",
            "Epoch 1, Batch 9800, Loss: 1.3563\n",
            "Epoch 1, Batch 9900, Loss: 1.4120\n",
            "Epoch 1, Batch 10000, Loss: 1.3408\n",
            "Epoch 1, Batch 10100, Loss: 1.3484\n",
            "Epoch 1, Batch 10200, Loss: 1.3472\n",
            "Epoch 1, Batch 10300, Loss: 1.3166\n",
            "Epoch 1, Batch 10400, Loss: 1.3124\n",
            "Epoch 1, Batch 10500, Loss: 1.3493\n",
            "Epoch 1, Batch 10600, Loss: 1.3457\n",
            "Epoch 1, Batch 10700, Loss: 1.3220\n",
            "Epoch 1, Batch 10800, Loss: 1.3588\n",
            "Epoch 1, Batch 10900, Loss: 1.3529\n",
            "Epoch 1, Batch 11000, Loss: 1.3326\n",
            "Epoch 1, Batch 11100, Loss: 1.2868\n",
            "Epoch 1, Batch 11200, Loss: 1.3358\n",
            "Epoch 1, Batch 11300, Loss: 1.3312\n",
            "Epoch 1, Batch 11400, Loss: 1.3455\n",
            "Epoch 1, Batch 11500, Loss: 1.3625\n",
            "Epoch 1, Batch 11600, Loss: 1.3915\n",
            "Epoch 1, Batch 11700, Loss: 1.3277\n",
            "Epoch 1, Batch 11800, Loss: 1.3492\n",
            "Epoch 1, Batch 11900, Loss: 1.3649\n",
            "Epoch 1, Batch 12000, Loss: 1.3193\n",
            "Epoch 1, Batch 12100, Loss: 1.3798\n",
            "Epoch 1, Batch 12200, Loss: 1.3545\n",
            "Epoch 1, Batch 12300, Loss: 1.3811\n",
            "Epoch 1, Batch 12400, Loss: 1.3109\n",
            "Epoch 1, Batch 12500, Loss: 1.3377\n",
            "Epoch 1, Batch 12600, Loss: 1.2828\n",
            "Epoch 1, Batch 12700, Loss: 1.3198\n",
            "Epoch 1, Batch 12800, Loss: 1.3634\n",
            "Epoch 1, Batch 12900, Loss: 1.3024\n",
            "Epoch 1, Batch 13000, Loss: 1.2893\n",
            "Epoch 1, Batch 13100, Loss: 1.3213\n",
            "Epoch 1, Batch 13200, Loss: 1.3087\n",
            "Epoch 1, Batch 13300, Loss: 1.3806\n",
            "Epoch 1, Batch 13400, Loss: 1.3313\n",
            "Epoch 1, Batch 13500, Loss: 1.3025\n",
            "Epoch 1, Batch 13600, Loss: 1.3249\n",
            "Epoch 1, Batch 13700, Loss: 1.3160\n",
            "Epoch 1, Batch 13800, Loss: 1.3483\n",
            "Epoch 1, Batch 13900, Loss: 1.3473\n",
            "Epoch 1, Batch 14000, Loss: 1.3285\n",
            "Epoch 1, Batch 14100, Loss: 1.3635\n",
            "Epoch 1, Batch 14200, Loss: 1.3046\n",
            "Epoch 1, Batch 14300, Loss: 1.3129\n",
            "Epoch 1, Batch 14400, Loss: 1.3146\n",
            "Epoch 1, Batch 14500, Loss: 1.3205\n",
            "Epoch 1, Batch 14600, Loss: 1.3231\n",
            "Epoch 1, Batch 14700, Loss: 1.3471\n",
            "Epoch 1, Batch 14800, Loss: 1.3342\n",
            "Epoch 1, Batch 14900, Loss: 1.3156\n",
            "Epoch 1, Batch 15000, Loss: 1.3307\n",
            "Epoch 1, Batch 15100, Loss: 1.3242\n",
            "Epoch 1, Batch 15200, Loss: 1.3070\n",
            "Epoch 1, Batch 15300, Loss: 1.2919\n",
            "Epoch 1, Batch 15400, Loss: 1.3381\n",
            "Epoch 1, Batch 15500, Loss: 1.3398\n",
            "Epoch 1, Batch 15600, Loss: 1.3040\n",
            "Epoch 1, Batch 15700, Loss: 1.3344\n",
            "Epoch 1, Batch 15800, Loss: 1.3131\n",
            "Epoch 1, Batch 15900, Loss: 1.3297\n",
            "Epoch 1, Batch 16000, Loss: 1.2959\n",
            "Epoch 1, Batch 16100, Loss: 1.3060\n",
            "Epoch 1, Batch 16200, Loss: 1.2921\n",
            "Epoch 1, Batch 16300, Loss: 1.3619\n",
            "Epoch 1, Batch 16400, Loss: 1.2971\n",
            "Epoch 1, Batch 16500, Loss: 1.2730\n",
            "Epoch 1, Batch 16600, Loss: 1.2999\n",
            "Epoch 1, Batch 16700, Loss: 1.3051\n",
            "Epoch 1, Batch 16800, Loss: 1.2931\n",
            "Epoch 1, Batch 16900, Loss: 1.3449\n",
            "Epoch 1, Batch 17000, Loss: 1.2940\n",
            "Epoch 1, Batch 17100, Loss: 1.3422\n",
            "Epoch 1, Batch 17200, Loss: 1.3610\n",
            "Epoch 1, Batch 17300, Loss: 1.3141\n",
            "Epoch 1, Batch 17400, Loss: 1.3178\n",
            "Epoch 1 completed, Average Loss: 1.3864\n",
            "Epoch 2, Batch 0, Loss: 1.3466\n",
            "Epoch 2, Batch 100, Loss: 1.3246\n",
            "Epoch 2, Batch 200, Loss: 1.3204\n",
            "Epoch 2, Batch 300, Loss: 1.3022\n",
            "Epoch 2, Batch 400, Loss: 1.3387\n",
            "Epoch 2, Batch 500, Loss: 1.3485\n",
            "Epoch 2, Batch 600, Loss: 1.3170\n",
            "Epoch 2, Batch 700, Loss: 1.3201\n",
            "Epoch 2, Batch 800, Loss: 1.3314\n",
            "Epoch 2, Batch 900, Loss: 1.2966\n",
            "Epoch 2, Batch 1000, Loss: 1.2923\n",
            "Epoch 2, Batch 1100, Loss: 1.2886\n",
            "Epoch 2, Batch 1200, Loss: 1.3155\n",
            "Epoch 2, Batch 1300, Loss: 1.3133\n",
            "Epoch 2, Batch 1400, Loss: 1.2946\n",
            "Epoch 2, Batch 1500, Loss: 1.3106\n",
            "Epoch 2, Batch 1600, Loss: 1.2954\n",
            "Epoch 2, Batch 1700, Loss: 1.3217\n",
            "Epoch 2, Batch 1800, Loss: 1.2630\n",
            "Epoch 2, Batch 1900, Loss: 1.3117\n",
            "Epoch 2, Batch 2000, Loss: 1.2932\n",
            "Epoch 2, Batch 2100, Loss: 1.2658\n",
            "Epoch 2, Batch 2200, Loss: 1.3033\n",
            "Epoch 2, Batch 2300, Loss: 1.2955\n",
            "Epoch 2, Batch 2400, Loss: 1.2893\n",
            "Epoch 2, Batch 2500, Loss: 1.3117\n",
            "Epoch 2, Batch 2600, Loss: 1.3338\n",
            "Epoch 2, Batch 2700, Loss: 1.3009\n",
            "Epoch 2, Batch 2800, Loss: 1.3114\n",
            "Epoch 2, Batch 2900, Loss: 1.2853\n",
            "Epoch 2, Batch 3000, Loss: 1.2989\n",
            "Epoch 2, Batch 3100, Loss: 1.3241\n",
            "Epoch 2, Batch 3200, Loss: 1.2993\n",
            "Epoch 2, Batch 3300, Loss: 1.3215\n",
            "Epoch 2, Batch 3400, Loss: 1.3114\n",
            "Epoch 2, Batch 3500, Loss: 1.2873\n",
            "Epoch 2, Batch 3600, Loss: 1.3398\n",
            "Epoch 2, Batch 3700, Loss: 1.2740\n",
            "Epoch 2, Batch 3800, Loss: 1.3094\n",
            "Epoch 2, Batch 3900, Loss: 1.2798\n",
            "Epoch 2, Batch 4000, Loss: 1.2845\n",
            "Epoch 2, Batch 4100, Loss: 1.3209\n",
            "Epoch 2, Batch 4200, Loss: 1.3285\n",
            "Epoch 2, Batch 4300, Loss: 1.3161\n",
            "Epoch 2, Batch 4400, Loss: 1.3001\n",
            "Epoch 2, Batch 4500, Loss: 1.3360\n",
            "Epoch 2, Batch 4600, Loss: 1.3297\n",
            "Epoch 2, Batch 4700, Loss: 1.3092\n",
            "Epoch 2, Batch 4800, Loss: 1.2665\n",
            "Epoch 2, Batch 4900, Loss: 1.3203\n",
            "Epoch 2, Batch 5000, Loss: 1.3303\n",
            "Epoch 2, Batch 5100, Loss: 1.3699\n",
            "Epoch 2, Batch 5200, Loss: 1.3354\n",
            "Epoch 2, Batch 5300, Loss: 1.2791\n",
            "Epoch 2, Batch 5400, Loss: 1.3071\n",
            "Epoch 2, Batch 5500, Loss: 1.3266\n",
            "Epoch 2, Batch 5600, Loss: 1.3195\n",
            "Epoch 2, Batch 5700, Loss: 1.3024\n",
            "Epoch 2, Batch 5800, Loss: 1.3276\n",
            "Epoch 2, Batch 5900, Loss: 1.3570\n",
            "Epoch 2, Batch 6000, Loss: 1.3047\n",
            "Epoch 2, Batch 6100, Loss: 1.3404\n",
            "Epoch 2, Batch 6200, Loss: 1.3276\n",
            "Epoch 2, Batch 6300, Loss: 1.3305\n",
            "Epoch 2, Batch 6400, Loss: 1.3065\n",
            "Epoch 2, Batch 6500, Loss: 1.3025\n",
            "Epoch 2, Batch 6600, Loss: 1.2876\n",
            "Epoch 2, Batch 6700, Loss: 1.3252\n",
            "Epoch 2, Batch 6800, Loss: 1.2928\n",
            "Epoch 2, Batch 6900, Loss: 1.3217\n",
            "Epoch 2, Batch 7000, Loss: 1.3127\n",
            "Epoch 2, Batch 7100, Loss: 1.3155\n",
            "Epoch 2, Batch 7200, Loss: 1.3160\n",
            "Epoch 2, Batch 7300, Loss: 1.2737\n",
            "Epoch 2, Batch 7400, Loss: 1.3219\n",
            "Epoch 2, Batch 7500, Loss: 1.3418\n",
            "Epoch 2, Batch 7600, Loss: 1.3463\n",
            "Epoch 2, Batch 7700, Loss: 1.3403\n",
            "Epoch 2, Batch 7800, Loss: 1.2949\n",
            "Epoch 2, Batch 7900, Loss: 1.2951\n",
            "Epoch 2, Batch 8000, Loss: 1.3214\n",
            "Epoch 2, Batch 8100, Loss: 1.3095\n",
            "Epoch 2, Batch 8200, Loss: 1.3137\n",
            "Epoch 2, Batch 8300, Loss: 1.2696\n",
            "Epoch 2, Batch 8400, Loss: 1.3002\n",
            "Epoch 2, Batch 8500, Loss: 1.3324\n",
            "Epoch 2, Batch 8600, Loss: 1.3153\n",
            "Epoch 2, Batch 8700, Loss: 1.3412\n",
            "Epoch 2, Batch 8800, Loss: 1.2919\n",
            "Epoch 2, Batch 8900, Loss: 1.2961\n",
            "Epoch 2, Batch 9000, Loss: 1.3380\n",
            "Epoch 2, Batch 9100, Loss: 1.2946\n",
            "Epoch 2, Batch 9200, Loss: 1.3017\n",
            "Epoch 2, Batch 9300, Loss: 1.3016\n",
            "Epoch 2, Batch 9400, Loss: 1.2913\n",
            "Epoch 2, Batch 9500, Loss: 1.3299\n",
            "Epoch 2, Batch 9600, Loss: 1.3209\n",
            "Epoch 2, Batch 9700, Loss: 1.2843\n",
            "Epoch 2, Batch 9800, Loss: 1.2645\n",
            "Epoch 2, Batch 9900, Loss: 1.2738\n",
            "Epoch 2, Batch 10000, Loss: 1.3217\n",
            "Epoch 2, Batch 10100, Loss: 1.2963\n",
            "Epoch 2, Batch 10200, Loss: 1.3140\n",
            "Epoch 2, Batch 10300, Loss: 1.3415\n",
            "Epoch 2, Batch 10400, Loss: 1.3228\n",
            "Epoch 2, Batch 10500, Loss: 1.3064\n",
            "Epoch 2, Batch 10600, Loss: 1.3056\n",
            "Epoch 2, Batch 10700, Loss: 1.3305\n",
            "Epoch 2, Batch 10800, Loss: 1.3137\n",
            "Epoch 2, Batch 10900, Loss: 1.2676\n",
            "Epoch 2, Batch 11000, Loss: 1.3421\n",
            "Epoch 2, Batch 11100, Loss: 1.2941\n",
            "Epoch 2, Batch 11200, Loss: 1.3238\n",
            "Epoch 2, Batch 11300, Loss: 1.3291\n",
            "Epoch 2, Batch 11400, Loss: 1.2894\n",
            "Epoch 2, Batch 11500, Loss: 1.3188\n",
            "Epoch 2, Batch 11600, Loss: 1.2835\n",
            "Epoch 2, Batch 11700, Loss: 1.2870\n",
            "Epoch 2, Batch 11800, Loss: 1.3810\n",
            "Epoch 2, Batch 11900, Loss: 1.3199\n",
            "Epoch 2, Batch 12000, Loss: 1.2970\n",
            "Epoch 2, Batch 12100, Loss: 1.2873\n",
            "Epoch 2, Batch 12200, Loss: 1.3302\n",
            "Epoch 2, Batch 12300, Loss: 1.3061\n",
            "Epoch 2, Batch 12400, Loss: 1.3060\n",
            "Epoch 2, Batch 12500, Loss: 1.3057\n",
            "Epoch 2, Batch 12600, Loss: 1.2814\n",
            "Epoch 2, Batch 12700, Loss: 1.2942\n",
            "Epoch 2, Batch 12800, Loss: 1.3103\n",
            "Epoch 2, Batch 12900, Loss: 1.2678\n",
            "Epoch 2, Batch 13000, Loss: 1.2916\n",
            "Epoch 2, Batch 13100, Loss: 1.3373\n",
            "Epoch 2, Batch 13200, Loss: 1.2882\n",
            "Epoch 2, Batch 13300, Loss: 1.3249\n",
            "Epoch 2, Batch 13400, Loss: 1.3048\n",
            "Epoch 2, Batch 13500, Loss: 1.3245\n",
            "Epoch 2, Batch 13600, Loss: 1.2500\n",
            "Epoch 2, Batch 13700, Loss: 1.3137\n",
            "Epoch 2, Batch 13800, Loss: 1.3354\n",
            "Epoch 2, Batch 13900, Loss: 1.3180\n",
            "Epoch 2, Batch 14000, Loss: 1.3193\n",
            "Epoch 2, Batch 14100, Loss: 1.2746\n",
            "Epoch 2, Batch 14200, Loss: 1.3103\n",
            "Epoch 2, Batch 14300, Loss: 1.3081\n",
            "Epoch 2, Batch 14400, Loss: 1.3007\n",
            "Epoch 2, Batch 14500, Loss: 1.3345\n",
            "Epoch 2, Batch 14600, Loss: 1.2871\n",
            "Epoch 2, Batch 14700, Loss: 1.2363\n",
            "Epoch 2, Batch 14800, Loss: 1.2835\n",
            "Epoch 2, Batch 14900, Loss: 1.2855\n",
            "Epoch 2, Batch 15000, Loss: 1.3144\n",
            "Epoch 2, Batch 15100, Loss: 1.2741\n",
            "Epoch 2, Batch 15200, Loss: 1.2664\n",
            "Epoch 2, Batch 15300, Loss: 1.3197\n",
            "Epoch 2, Batch 15400, Loss: 1.2911\n",
            "Epoch 2, Batch 15500, Loss: 1.2619\n",
            "Epoch 2, Batch 15600, Loss: 1.3168\n",
            "Epoch 2, Batch 15700, Loss: 1.2840\n",
            "Epoch 2, Batch 15800, Loss: 1.2646\n",
            "Epoch 2, Batch 15900, Loss: 1.2994\n",
            "Epoch 2, Batch 16000, Loss: 1.3045\n",
            "Epoch 2, Batch 16100, Loss: 1.2694\n",
            "Epoch 2, Batch 16200, Loss: 1.3270\n",
            "Epoch 2, Batch 16300, Loss: 1.3112\n",
            "Epoch 2, Batch 16400, Loss: 1.3261\n",
            "Epoch 2, Batch 16500, Loss: 1.3389\n",
            "Epoch 2, Batch 16600, Loss: 1.3099\n",
            "Epoch 2, Batch 16700, Loss: 1.2521\n",
            "Epoch 2, Batch 16800, Loss: 1.2703\n",
            "Epoch 2, Batch 16900, Loss: 1.2871\n",
            "Epoch 2, Batch 17000, Loss: 1.3327\n",
            "Epoch 2, Batch 17100, Loss: 1.2715\n",
            "Epoch 2, Batch 17200, Loss: 1.2570\n",
            "Epoch 2, Batch 17300, Loss: 1.3073\n",
            "Epoch 2, Batch 17400, Loss: 1.3382\n",
            "Epoch 2 completed, Average Loss: 1.3089\n",
            "Epoch 3, Batch 0, Loss: 1.3242\n",
            "Epoch 3, Batch 100, Loss: 1.2716\n",
            "Epoch 3, Batch 200, Loss: 1.2814\n",
            "Epoch 3, Batch 300, Loss: 1.3214\n",
            "Epoch 3, Batch 400, Loss: 1.3413\n",
            "Epoch 3, Batch 500, Loss: 1.2951\n",
            "Epoch 3, Batch 600, Loss: 1.2949\n",
            "Epoch 3, Batch 700, Loss: 1.2843\n",
            "Epoch 3, Batch 800, Loss: 1.3164\n",
            "Epoch 3, Batch 900, Loss: 1.2789\n",
            "Epoch 3, Batch 1000, Loss: 1.3096\n",
            "Epoch 3, Batch 1100, Loss: 1.2621\n",
            "Epoch 3, Batch 1200, Loss: 1.2988\n",
            "Epoch 3, Batch 1300, Loss: 1.3163\n",
            "Epoch 3, Batch 1400, Loss: 1.2639\n",
            "Epoch 3, Batch 1500, Loss: 1.2660\n",
            "Epoch 3, Batch 1600, Loss: 1.3075\n",
            "Epoch 3, Batch 1700, Loss: 1.2759\n",
            "Epoch 3, Batch 1800, Loss: 1.2805\n",
            "Epoch 3, Batch 1900, Loss: 1.2813\n",
            "Epoch 3, Batch 2000, Loss: 1.2920\n",
            "Epoch 3, Batch 2100, Loss: 1.3055\n",
            "Epoch 3, Batch 2200, Loss: 1.2878\n",
            "Epoch 3, Batch 2300, Loss: 1.2598\n",
            "Epoch 3, Batch 2400, Loss: 1.2736\n",
            "Epoch 3, Batch 2500, Loss: 1.2944\n",
            "Epoch 3, Batch 2600, Loss: 1.3199\n",
            "Epoch 3, Batch 2700, Loss: 1.2997\n",
            "Epoch 3, Batch 2800, Loss: 1.2638\n",
            "Epoch 3, Batch 2900, Loss: 1.3109\n",
            "Epoch 3, Batch 3000, Loss: 1.3188\n",
            "Epoch 3, Batch 3100, Loss: 1.3296\n",
            "Epoch 3, Batch 3200, Loss: 1.3568\n",
            "Epoch 3, Batch 3300, Loss: 1.2915\n",
            "Epoch 3, Batch 3400, Loss: 1.3257\n",
            "Epoch 3, Batch 3500, Loss: 1.3065\n",
            "Epoch 3, Batch 3600, Loss: 1.2930\n",
            "Epoch 3, Batch 3700, Loss: 1.3160\n",
            "Epoch 3, Batch 3800, Loss: 1.3155\n",
            "Epoch 3, Batch 3900, Loss: 1.3157\n",
            "Epoch 3, Batch 4000, Loss: 1.2843\n",
            "Epoch 3, Batch 4100, Loss: 1.2673\n",
            "Epoch 3, Batch 4200, Loss: 1.2917\n",
            "Epoch 3, Batch 4300, Loss: 1.3034\n",
            "Epoch 3, Batch 4400, Loss: 1.3379\n",
            "Epoch 3, Batch 4500, Loss: 1.2905\n",
            "Epoch 3, Batch 4600, Loss: 1.2809\n",
            "Epoch 3, Batch 4700, Loss: 1.2448\n",
            "Epoch 3, Batch 4800, Loss: 1.3056\n",
            "Epoch 3, Batch 4900, Loss: 1.2991\n",
            "Epoch 3, Batch 5000, Loss: 1.2506\n",
            "Epoch 3, Batch 5100, Loss: 1.3020\n",
            "Epoch 3, Batch 5200, Loss: 1.2938\n",
            "Epoch 3, Batch 5300, Loss: 1.3080\n",
            "Epoch 3, Batch 5400, Loss: 1.2666\n",
            "Epoch 3, Batch 5500, Loss: 1.2811\n",
            "Epoch 3, Batch 5600, Loss: 1.3005\n",
            "Epoch 3, Batch 5700, Loss: 1.3149\n",
            "Epoch 3, Batch 5800, Loss: 1.2599\n",
            "Epoch 3, Batch 5900, Loss: 1.3278\n",
            "Epoch 3, Batch 6000, Loss: 1.2940\n",
            "Epoch 3, Batch 6100, Loss: 1.3042\n",
            "Epoch 3, Batch 6200, Loss: 1.3066\n",
            "Epoch 3, Batch 6300, Loss: 1.2944\n",
            "Epoch 3, Batch 6400, Loss: 1.2753\n",
            "Epoch 3, Batch 6500, Loss: 1.3112\n",
            "Epoch 3, Batch 6600, Loss: 1.3033\n",
            "Epoch 3, Batch 6700, Loss: 1.3193\n",
            "Epoch 3, Batch 6800, Loss: 1.2837\n",
            "Epoch 3, Batch 6900, Loss: 1.2787\n",
            "Epoch 3, Batch 7000, Loss: 1.3397\n",
            "Epoch 3, Batch 7100, Loss: 1.3033\n",
            "Epoch 3, Batch 7200, Loss: 1.2890\n",
            "Epoch 3, Batch 7300, Loss: 1.2863\n",
            "Epoch 3, Batch 7400, Loss: 1.2886\n",
            "Epoch 3, Batch 7500, Loss: 1.2933\n",
            "Epoch 3, Batch 7600, Loss: 1.3342\n",
            "Epoch 3, Batch 7700, Loss: 1.3070\n",
            "Epoch 3, Batch 7800, Loss: 1.2659\n",
            "Epoch 3, Batch 7900, Loss: 1.3102\n",
            "Epoch 3, Batch 8000, Loss: 1.3122\n",
            "Epoch 3, Batch 8100, Loss: 1.3070\n",
            "Epoch 3, Batch 8200, Loss: 1.2743\n",
            "Epoch 3, Batch 8300, Loss: 1.3066\n",
            "Epoch 3, Batch 8400, Loss: 1.2672\n",
            "Epoch 3, Batch 8500, Loss: 1.3085\n",
            "Epoch 3, Batch 8600, Loss: 1.3229\n",
            "Epoch 3, Batch 8700, Loss: 1.3203\n",
            "Epoch 3, Batch 8800, Loss: 1.3013\n",
            "Epoch 3, Batch 8900, Loss: 1.3080\n",
            "Epoch 3, Batch 9000, Loss: 1.2764\n",
            "Epoch 3, Batch 9100, Loss: 1.2905\n",
            "Epoch 3, Batch 9200, Loss: 1.2984\n",
            "Epoch 3, Batch 9300, Loss: 1.3090\n",
            "Epoch 3, Batch 9400, Loss: 1.3348\n",
            "Epoch 3, Batch 9500, Loss: 1.2981\n",
            "Epoch 3, Batch 9600, Loss: 1.2956\n",
            "Epoch 3, Batch 9700, Loss: 1.3141\n",
            "Epoch 3, Batch 9800, Loss: 1.3022\n",
            "Epoch 3, Batch 9900, Loss: 1.2516\n",
            "Epoch 3, Batch 10000, Loss: 1.2702\n",
            "Epoch 3, Batch 10100, Loss: 1.2679\n",
            "Epoch 3, Batch 10200, Loss: 1.3469\n",
            "Epoch 3, Batch 10300, Loss: 1.3005\n",
            "Epoch 3, Batch 10400, Loss: 1.2936\n",
            "Epoch 3, Batch 10500, Loss: 1.2710\n",
            "Epoch 3, Batch 10600, Loss: 1.2866\n",
            "Epoch 3, Batch 10700, Loss: 1.2944\n",
            "Epoch 3, Batch 10800, Loss: 1.3164\n",
            "Epoch 3, Batch 10900, Loss: 1.2805\n",
            "Epoch 3, Batch 11000, Loss: 1.2986\n",
            "Epoch 3, Batch 11100, Loss: 1.3272\n",
            "Epoch 3, Batch 11200, Loss: 1.2845\n",
            "Epoch 3, Batch 11300, Loss: 1.3069\n",
            "Epoch 3, Batch 11400, Loss: 1.3125\n",
            "Epoch 3, Batch 11500, Loss: 1.2883\n",
            "Epoch 3, Batch 11600, Loss: 1.2534\n",
            "Epoch 3, Batch 11700, Loss: 1.2583\n",
            "Epoch 3, Batch 11800, Loss: 1.3215\n",
            "Epoch 3, Batch 11900, Loss: 1.3415\n",
            "Epoch 3, Batch 12000, Loss: 1.3124\n",
            "Epoch 3, Batch 12100, Loss: 1.3353\n",
            "Epoch 3, Batch 12200, Loss: 1.2788\n",
            "Epoch 3, Batch 12300, Loss: 1.2841\n",
            "Epoch 3, Batch 12400, Loss: 1.2836\n",
            "Epoch 3, Batch 12500, Loss: 1.2730\n",
            "Epoch 3, Batch 12600, Loss: 1.2551\n",
            "Epoch 3, Batch 12700, Loss: 1.3054\n",
            "Epoch 3, Batch 12800, Loss: 1.2890\n",
            "Epoch 3, Batch 12900, Loss: 1.3187\n",
            "Epoch 3, Batch 13000, Loss: 1.3079\n",
            "Epoch 3, Batch 13100, Loss: 1.2934\n",
            "Epoch 3, Batch 13200, Loss: 1.3018\n",
            "Epoch 3, Batch 13300, Loss: 1.3013\n",
            "Epoch 3, Batch 13400, Loss: 1.2968\n",
            "Epoch 3, Batch 13500, Loss: 1.3029\n",
            "Epoch 3, Batch 13600, Loss: 1.2414\n",
            "Epoch 3, Batch 13700, Loss: 1.3054\n",
            "Epoch 3, Batch 13800, Loss: 1.2746\n",
            "Epoch 3, Batch 13900, Loss: 1.3045\n",
            "Epoch 3, Batch 14000, Loss: 1.3109\n",
            "Epoch 3, Batch 14100, Loss: 1.3201\n",
            "Epoch 3, Batch 14200, Loss: 1.2917\n",
            "Epoch 3, Batch 14300, Loss: 1.2733\n",
            "Epoch 3, Batch 14400, Loss: 1.3220\n",
            "Epoch 3, Batch 14500, Loss: 1.2999\n",
            "Epoch 3, Batch 14600, Loss: 1.2699\n",
            "Epoch 3, Batch 14700, Loss: 1.3212\n",
            "Epoch 3, Batch 14800, Loss: 1.2892\n",
            "Epoch 3, Batch 14900, Loss: 1.2850\n",
            "Epoch 3, Batch 15000, Loss: 1.2387\n",
            "Epoch 3, Batch 15100, Loss: 1.2696\n",
            "Epoch 3, Batch 15200, Loss: 1.2902\n",
            "Epoch 3, Batch 15300, Loss: 1.2799\n",
            "Epoch 3, Batch 15400, Loss: 1.2877\n",
            "Epoch 3, Batch 15500, Loss: 1.3029\n",
            "Epoch 3, Batch 15600, Loss: 1.2864\n",
            "Epoch 3, Batch 15700, Loss: 1.2965\n",
            "Epoch 3, Batch 15800, Loss: 1.3092\n",
            "Epoch 3, Batch 15900, Loss: 1.2605\n",
            "Epoch 3, Batch 16000, Loss: 1.2775\n",
            "Epoch 3, Batch 16100, Loss: 1.2941\n",
            "Epoch 3, Batch 16200, Loss: 1.2808\n",
            "Epoch 3, Batch 16300, Loss: 1.2903\n",
            "Epoch 3, Batch 16400, Loss: 1.3007\n",
            "Epoch 3, Batch 16500, Loss: 1.2507\n",
            "Epoch 3, Batch 16600, Loss: 1.2925\n",
            "Epoch 3, Batch 16700, Loss: 1.3115\n",
            "Epoch 3, Batch 16800, Loss: 1.3264\n",
            "Epoch 3, Batch 16900, Loss: 1.2946\n",
            "Epoch 3, Batch 17000, Loss: 1.3145\n",
            "Epoch 3, Batch 17100, Loss: 1.2990\n",
            "Epoch 3, Batch 17200, Loss: 1.2712\n",
            "Epoch 3, Batch 17300, Loss: 1.3152\n",
            "Epoch 3, Batch 17400, Loss: 1.2905\n",
            "Epoch 3 completed, Average Loss: 1.2984\n",
            "Epoch 4, Batch 0, Loss: 1.3056\n",
            "Epoch 4, Batch 100, Loss: 1.3013\n",
            "Epoch 4, Batch 200, Loss: 1.3326\n",
            "Epoch 4, Batch 300, Loss: 1.2814\n",
            "Epoch 4, Batch 400, Loss: 1.2838\n",
            "Epoch 4, Batch 500, Loss: 1.2734\n",
            "Epoch 4, Batch 600, Loss: 1.2968\n",
            "Epoch 4, Batch 700, Loss: 1.2957\n",
            "Epoch 4, Batch 800, Loss: 1.2695\n",
            "Epoch 4, Batch 900, Loss: 1.3115\n",
            "Epoch 4, Batch 1000, Loss: 1.2901\n",
            "Epoch 4, Batch 1100, Loss: 1.3222\n",
            "Epoch 4, Batch 1200, Loss: 1.2899\n",
            "Epoch 4, Batch 1300, Loss: 1.3221\n",
            "Epoch 4, Batch 1400, Loss: 1.2880\n",
            "Epoch 4, Batch 1500, Loss: 1.3451\n",
            "Epoch 4, Batch 1600, Loss: 1.2985\n",
            "Epoch 4, Batch 1700, Loss: 1.3192\n",
            "Epoch 4, Batch 1800, Loss: 1.2910\n",
            "Epoch 4, Batch 1900, Loss: 1.2529\n",
            "Epoch 4, Batch 2000, Loss: 1.2744\n",
            "Epoch 4, Batch 2100, Loss: 1.2975\n",
            "Epoch 4, Batch 2200, Loss: 1.2661\n",
            "Epoch 4, Batch 2300, Loss: 1.3297\n",
            "Epoch 4, Batch 2400, Loss: 1.3325\n",
            "Epoch 4, Batch 2500, Loss: 1.2489\n",
            "Epoch 4, Batch 2600, Loss: 1.3131\n",
            "Epoch 4, Batch 2700, Loss: 1.3343\n",
            "Epoch 4, Batch 2800, Loss: 1.2745\n",
            "Epoch 4, Batch 2900, Loss: 1.2726\n",
            "Epoch 4, Batch 3000, Loss: 1.2633\n",
            "Epoch 4, Batch 3100, Loss: 1.2957\n",
            "Epoch 4, Batch 3200, Loss: 1.2952\n",
            "Epoch 4, Batch 3300, Loss: 1.2979\n",
            "Epoch 4, Batch 3400, Loss: 1.3081\n",
            "Epoch 4, Batch 3500, Loss: 1.2913\n",
            "Epoch 4, Batch 3600, Loss: 1.2803\n",
            "Epoch 4, Batch 3700, Loss: 1.3057\n",
            "Epoch 4, Batch 3800, Loss: 1.3236\n",
            "Epoch 4, Batch 3900, Loss: 1.2924\n",
            "Epoch 4, Batch 4000, Loss: 1.2638\n",
            "Epoch 4, Batch 4100, Loss: 1.3394\n",
            "Epoch 4, Batch 4200, Loss: 1.2830\n",
            "Epoch 4, Batch 4300, Loss: 1.2956\n",
            "Epoch 4, Batch 4400, Loss: 1.2781\n",
            "Epoch 4, Batch 4500, Loss: 1.2919\n",
            "Epoch 4, Batch 4600, Loss: 1.2926\n",
            "Epoch 4, Batch 4700, Loss: 1.3201\n",
            "Epoch 4, Batch 4800, Loss: 1.2886\n",
            "Epoch 4, Batch 4900, Loss: 1.2627\n",
            "Epoch 4, Batch 5000, Loss: 1.2849\n",
            "Epoch 4, Batch 5100, Loss: 1.3120\n",
            "Epoch 4, Batch 5200, Loss: 1.2961\n",
            "Epoch 4, Batch 5300, Loss: 1.3047\n",
            "Epoch 4, Batch 5400, Loss: 1.2540\n",
            "Epoch 4, Batch 5500, Loss: 1.2976\n",
            "Epoch 4, Batch 5600, Loss: 1.2936\n",
            "Epoch 4, Batch 5700, Loss: 1.2637\n",
            "Epoch 4, Batch 5800, Loss: 1.2918\n",
            "Epoch 4, Batch 5900, Loss: 1.2827\n",
            "Epoch 4, Batch 6000, Loss: 1.2976\n",
            "Epoch 4, Batch 6100, Loss: 1.3420\n",
            "Epoch 4, Batch 6200, Loss: 1.2848\n",
            "Epoch 4, Batch 6300, Loss: 1.2788\n",
            "Epoch 4, Batch 6400, Loss: 1.2952\n",
            "Epoch 4, Batch 6500, Loss: 1.2746\n",
            "Epoch 4, Batch 6600, Loss: 1.2829\n",
            "Epoch 4, Batch 6700, Loss: 1.2623\n",
            "Epoch 4, Batch 6800, Loss: 1.2923\n",
            "Epoch 4, Batch 6900, Loss: 1.3072\n",
            "Epoch 4, Batch 7000, Loss: 1.3172\n",
            "Epoch 4, Batch 7100, Loss: 1.2788\n",
            "Epoch 4, Batch 7200, Loss: 1.3058\n",
            "Epoch 4, Batch 7300, Loss: 1.2988\n",
            "Epoch 4, Batch 7400, Loss: 1.3023\n",
            "Epoch 4, Batch 7500, Loss: 1.2864\n",
            "Epoch 4, Batch 7600, Loss: 1.3142\n",
            "Epoch 4, Batch 7700, Loss: 1.2907\n",
            "Epoch 4, Batch 7800, Loss: 1.3172\n",
            "Epoch 4, Batch 7900, Loss: 1.3014\n",
            "Epoch 4, Batch 8000, Loss: 1.2933\n",
            "Epoch 4, Batch 8100, Loss: 1.2750\n",
            "Epoch 4, Batch 8200, Loss: 1.2928\n",
            "Epoch 4, Batch 8300, Loss: 1.3285\n",
            "Epoch 4, Batch 8400, Loss: 1.2541\n",
            "Epoch 4, Batch 8500, Loss: 1.3102\n",
            "Epoch 4, Batch 8600, Loss: 1.2752\n",
            "Epoch 4, Batch 8700, Loss: 1.2668\n",
            "Epoch 4, Batch 8800, Loss: 1.3120\n",
            "Epoch 4, Batch 8900, Loss: 1.2718\n",
            "Epoch 4, Batch 9000, Loss: 1.2968\n",
            "Epoch 4, Batch 9100, Loss: 1.3114\n",
            "Epoch 4, Batch 9200, Loss: 1.2984\n",
            "Epoch 4, Batch 9300, Loss: 1.2828\n",
            "Epoch 4, Batch 9400, Loss: 1.2967\n",
            "Epoch 4, Batch 9500, Loss: 1.2915\n",
            "Epoch 4, Batch 9600, Loss: 1.3307\n",
            "Epoch 4, Batch 9700, Loss: 1.2952\n",
            "Epoch 4, Batch 9800, Loss: 1.2717\n",
            "Epoch 4, Batch 9900, Loss: 1.2694\n",
            "Epoch 4, Batch 10000, Loss: 1.2868\n",
            "Epoch 4, Batch 10100, Loss: 1.3087\n",
            "Epoch 4, Batch 10200, Loss: 1.2980\n",
            "Epoch 4, Batch 10300, Loss: 1.3007\n",
            "Epoch 4, Batch 10400, Loss: 1.3021\n",
            "Epoch 4, Batch 10500, Loss: 1.2915\n",
            "Epoch 4, Batch 10600, Loss: 1.2622\n",
            "Epoch 4, Batch 10700, Loss: 1.3095\n",
            "Epoch 4, Batch 10800, Loss: 1.2802\n",
            "Epoch 4, Batch 10900, Loss: 1.2847\n",
            "Epoch 4, Batch 11000, Loss: 1.2642\n",
            "Epoch 4, Batch 11100, Loss: 1.2621\n",
            "Epoch 4, Batch 11200, Loss: 1.2741\n",
            "Epoch 4, Batch 11300, Loss: 1.2519\n",
            "Epoch 4, Batch 11400, Loss: 1.3206\n",
            "Epoch 4, Batch 11500, Loss: 1.2693\n",
            "Epoch 4, Batch 11600, Loss: 1.2877\n",
            "Epoch 4, Batch 11700, Loss: 1.2695\n",
            "Epoch 4, Batch 11800, Loss: 1.2956\n",
            "Epoch 4, Batch 11900, Loss: 1.2631\n",
            "Epoch 4, Batch 12000, Loss: 1.2594\n",
            "Epoch 4, Batch 12100, Loss: 1.3090\n",
            "Epoch 4, Batch 12200, Loss: 1.2749\n",
            "Epoch 4, Batch 12300, Loss: 1.2723\n",
            "Epoch 4, Batch 12400, Loss: 1.2896\n",
            "Epoch 4, Batch 12500, Loss: 1.2903\n",
            "Epoch 4, Batch 12600, Loss: 1.3077\n",
            "Epoch 4, Batch 12700, Loss: 1.3165\n",
            "Epoch 4, Batch 12800, Loss: 1.2943\n",
            "Epoch 4, Batch 12900, Loss: 1.2646\n",
            "Epoch 4, Batch 13000, Loss: 1.3088\n",
            "Epoch 4, Batch 13100, Loss: 1.2295\n",
            "Epoch 4, Batch 13200, Loss: 1.3169\n",
            "Epoch 4, Batch 13300, Loss: 1.2921\n",
            "Epoch 4, Batch 13400, Loss: 1.2892\n",
            "Epoch 4, Batch 13500, Loss: 1.2742\n",
            "Epoch 4, Batch 13600, Loss: 1.3048\n",
            "Epoch 4, Batch 13700, Loss: 1.3017\n",
            "Epoch 4, Batch 13800, Loss: 1.2848\n",
            "Epoch 4, Batch 13900, Loss: 1.3026\n",
            "Epoch 4, Batch 14000, Loss: 1.2869\n",
            "Epoch 4, Batch 14100, Loss: 1.3217\n",
            "Epoch 4, Batch 14200, Loss: 1.2975\n",
            "Epoch 4, Batch 14300, Loss: 1.2540\n",
            "Epoch 4, Batch 14400, Loss: 1.2854\n",
            "Epoch 4, Batch 14500, Loss: 1.3049\n",
            "Epoch 4, Batch 14600, Loss: 1.2858\n",
            "Epoch 4, Batch 14700, Loss: 1.2905\n",
            "Epoch 4, Batch 14800, Loss: 1.2563\n",
            "Epoch 4, Batch 14900, Loss: 1.2653\n",
            "Epoch 4, Batch 15000, Loss: 1.3043\n",
            "Epoch 4, Batch 15100, Loss: 1.3010\n",
            "Epoch 4, Batch 15200, Loss: 1.2419\n",
            "Epoch 4, Batch 15300, Loss: 1.2384\n",
            "Epoch 4, Batch 15400, Loss: 1.3049\n",
            "Epoch 4, Batch 15500, Loss: 1.2907\n",
            "Epoch 4, Batch 15600, Loss: 1.3385\n",
            "Epoch 4, Batch 15700, Loss: 1.2835\n",
            "Epoch 4, Batch 15800, Loss: 1.2873\n",
            "Epoch 4, Batch 15900, Loss: 1.2905\n",
            "Epoch 4, Batch 16000, Loss: 1.3169\n",
            "Epoch 4, Batch 16100, Loss: 1.2913\n",
            "Epoch 4, Batch 16200, Loss: 1.2954\n",
            "Epoch 4, Batch 16300, Loss: 1.3101\n",
            "Epoch 4, Batch 16400, Loss: 1.3139\n",
            "Epoch 4, Batch 16500, Loss: 1.2906\n",
            "Epoch 4, Batch 16600, Loss: 1.2674\n",
            "Epoch 4, Batch 16700, Loss: 1.3017\n",
            "Epoch 4, Batch 16800, Loss: 1.2901\n",
            "Epoch 4, Batch 16900, Loss: 1.3223\n",
            "Epoch 4, Batch 17000, Loss: 1.3149\n",
            "Epoch 4, Batch 17100, Loss: 1.2791\n",
            "Epoch 4, Batch 17200, Loss: 1.2642\n",
            "Epoch 4, Batch 17300, Loss: 1.2831\n",
            "Epoch 4, Batch 17400, Loss: 1.2868\n",
            "Epoch 4 completed, Average Loss: 1.2937\n",
            "Epoch 5, Batch 0, Loss: 1.2761\n",
            "Epoch 5, Batch 100, Loss: 1.2859\n",
            "Epoch 5, Batch 200, Loss: 1.2857\n",
            "Epoch 5, Batch 300, Loss: 1.2836\n",
            "Epoch 5, Batch 400, Loss: 1.2593\n",
            "Epoch 5, Batch 500, Loss: 1.2759\n",
            "Epoch 5, Batch 600, Loss: 1.2807\n",
            "Epoch 5, Batch 700, Loss: 1.2842\n",
            "Epoch 5, Batch 800, Loss: 1.2873\n",
            "Epoch 5, Batch 900, Loss: 1.2909\n",
            "Epoch 5, Batch 1000, Loss: 1.2613\n",
            "Epoch 5, Batch 1100, Loss: 1.2736\n",
            "Epoch 5, Batch 1200, Loss: 1.3063\n",
            "Epoch 5, Batch 1300, Loss: 1.2901\n",
            "Epoch 5, Batch 1400, Loss: 1.2652\n",
            "Epoch 5, Batch 1500, Loss: 1.3113\n",
            "Epoch 5, Batch 1600, Loss: 1.2958\n",
            "Epoch 5, Batch 1700, Loss: 1.2922\n",
            "Epoch 5, Batch 1800, Loss: 1.2847\n",
            "Epoch 5, Batch 1900, Loss: 1.3452\n",
            "Epoch 5, Batch 2000, Loss: 1.2813\n",
            "Epoch 5, Batch 2100, Loss: 1.3167\n",
            "Epoch 5, Batch 2200, Loss: 1.3052\n",
            "Epoch 5, Batch 2300, Loss: 1.2982\n",
            "Epoch 5, Batch 2400, Loss: 1.2601\n",
            "Epoch 5, Batch 2500, Loss: 1.2498\n",
            "Epoch 5, Batch 2600, Loss: 1.2945\n",
            "Epoch 5, Batch 2700, Loss: 1.2670\n",
            "Epoch 5, Batch 2800, Loss: 1.3029\n",
            "Epoch 5, Batch 2900, Loss: 1.3082\n",
            "Epoch 5, Batch 3000, Loss: 1.2835\n",
            "Epoch 5, Batch 3100, Loss: 1.2812\n",
            "Epoch 5, Batch 3200, Loss: 1.3034\n",
            "Epoch 5, Batch 3300, Loss: 1.2877\n",
            "Epoch 5, Batch 3400, Loss: 1.2746\n",
            "Epoch 5, Batch 3500, Loss: 1.2867\n",
            "Epoch 5, Batch 3600, Loss: 1.2732\n",
            "Epoch 5, Batch 3700, Loss: 1.2882\n",
            "Epoch 5, Batch 3800, Loss: 1.2914\n",
            "Epoch 5, Batch 3900, Loss: 1.2895\n",
            "Epoch 5, Batch 4000, Loss: 1.3105\n",
            "Epoch 5, Batch 4100, Loss: 1.2728\n",
            "Epoch 5, Batch 4200, Loss: 1.2731\n",
            "Epoch 5, Batch 4300, Loss: 1.2987\n",
            "Epoch 5, Batch 4400, Loss: 1.3361\n",
            "Epoch 5, Batch 4500, Loss: 1.3244\n",
            "Epoch 5, Batch 4600, Loss: 1.2926\n",
            "Epoch 5, Batch 4700, Loss: 1.2747\n",
            "Epoch 5, Batch 4800, Loss: 1.2974\n",
            "Epoch 5, Batch 4900, Loss: 1.3501\n",
            "Epoch 5, Batch 5000, Loss: 1.2865\n",
            "Epoch 5, Batch 5100, Loss: 1.3151\n",
            "Epoch 5, Batch 5200, Loss: 1.2855\n",
            "Epoch 5, Batch 5300, Loss: 1.2925\n",
            "Epoch 5, Batch 5400, Loss: 1.2852\n",
            "Epoch 5, Batch 5500, Loss: 1.2730\n",
            "Epoch 5, Batch 5600, Loss: 1.3049\n",
            "Epoch 5, Batch 5700, Loss: 1.2655\n",
            "Epoch 5, Batch 5800, Loss: 1.2746\n",
            "Epoch 5, Batch 5900, Loss: 1.2991\n",
            "Epoch 5, Batch 6000, Loss: 1.2875\n",
            "Epoch 5, Batch 6100, Loss: 1.2811\n",
            "Epoch 5, Batch 6200, Loss: 1.2984\n",
            "Epoch 5, Batch 6300, Loss: 1.2964\n",
            "Epoch 5, Batch 6400, Loss: 1.2800\n",
            "Epoch 5, Batch 6500, Loss: 1.2741\n",
            "Epoch 5, Batch 6600, Loss: 1.3016\n",
            "Epoch 5, Batch 6700, Loss: 1.2929\n",
            "Epoch 5, Batch 6800, Loss: 1.2662\n",
            "Epoch 5, Batch 6900, Loss: 1.2910\n",
            "Epoch 5, Batch 7000, Loss: 1.3107\n",
            "Epoch 5, Batch 7100, Loss: 1.2960\n",
            "Epoch 5, Batch 7200, Loss: 1.3063\n",
            "Epoch 5, Batch 7300, Loss: 1.2729\n",
            "Epoch 5, Batch 7400, Loss: 1.2832\n",
            "Epoch 5, Batch 7500, Loss: 1.2661\n",
            "Epoch 5, Batch 7600, Loss: 1.3377\n",
            "Epoch 5, Batch 7700, Loss: 1.3233\n",
            "Epoch 5, Batch 7800, Loss: 1.3014\n",
            "Epoch 5, Batch 7900, Loss: 1.3210\n",
            "Epoch 5, Batch 8000, Loss: 1.2861\n",
            "Epoch 5, Batch 8100, Loss: 1.2923\n",
            "Epoch 5, Batch 8200, Loss: 1.2756\n",
            "Epoch 5, Batch 8300, Loss: 1.3188\n",
            "Epoch 5, Batch 8400, Loss: 1.2931\n",
            "Epoch 5, Batch 8500, Loss: 1.3135\n",
            "Epoch 5, Batch 8600, Loss: 1.2993\n",
            "Epoch 5, Batch 8700, Loss: 1.3047\n",
            "Epoch 5, Batch 8800, Loss: 1.3056\n",
            "Epoch 5, Batch 8900, Loss: 1.2657\n",
            "Epoch 5, Batch 9000, Loss: 1.2949\n",
            "Epoch 5, Batch 9100, Loss: 1.3077\n",
            "Epoch 5, Batch 9200, Loss: 1.2630\n",
            "Epoch 5, Batch 9300, Loss: 1.3102\n",
            "Epoch 5, Batch 9400, Loss: 1.2889\n",
            "Epoch 5, Batch 9500, Loss: 1.2783\n",
            "Epoch 5, Batch 9600, Loss: 1.3186\n",
            "Epoch 5, Batch 9700, Loss: 1.2869\n",
            "Epoch 5, Batch 9800, Loss: 1.2664\n",
            "Epoch 5, Batch 9900, Loss: 1.2613\n",
            "Epoch 5, Batch 10000, Loss: 1.2778\n",
            "Epoch 5, Batch 10100, Loss: 1.3124\n",
            "Epoch 5, Batch 10200, Loss: 1.2708\n",
            "Epoch 5, Batch 10300, Loss: 1.2809\n",
            "Epoch 5, Batch 10400, Loss: 1.3135\n",
            "Epoch 5, Batch 10500, Loss: 1.2956\n",
            "Epoch 5, Batch 10600, Loss: 1.3052\n",
            "Epoch 5, Batch 10700, Loss: 1.2929\n",
            "Epoch 5, Batch 10800, Loss: 1.2583\n",
            "Epoch 5, Batch 10900, Loss: 1.3462\n",
            "Epoch 5, Batch 11000, Loss: 1.2693\n",
            "Epoch 5, Batch 11100, Loss: 1.3209\n",
            "Epoch 5, Batch 11200, Loss: 1.3152\n",
            "Epoch 5, Batch 11300, Loss: 1.2884\n",
            "Epoch 5, Batch 11400, Loss: 1.2976\n",
            "Epoch 5, Batch 11500, Loss: 1.2910\n",
            "Epoch 5, Batch 11600, Loss: 1.3015\n",
            "Epoch 5, Batch 11700, Loss: 1.3312\n",
            "Epoch 5, Batch 11800, Loss: 1.2961\n",
            "Epoch 5, Batch 11900, Loss: 1.2717\n",
            "Epoch 5, Batch 12000, Loss: 1.2588\n",
            "Epoch 5, Batch 12100, Loss: 1.3125\n",
            "Epoch 5, Batch 12200, Loss: 1.2911\n",
            "Epoch 5, Batch 12300, Loss: 1.3033\n",
            "Epoch 5, Batch 12400, Loss: 1.2836\n",
            "Epoch 5, Batch 12500, Loss: 1.2887\n",
            "Epoch 5, Batch 12600, Loss: 1.2843\n",
            "Epoch 5, Batch 12700, Loss: 1.2801\n",
            "Epoch 5, Batch 12800, Loss: 1.3122\n",
            "Epoch 5, Batch 12900, Loss: 1.3051\n",
            "Epoch 5, Batch 13000, Loss: 1.2694\n",
            "Epoch 5, Batch 13100, Loss: 1.2987\n",
            "Epoch 5, Batch 13200, Loss: 1.2717\n",
            "Epoch 5, Batch 13300, Loss: 1.2992\n",
            "Epoch 5, Batch 13400, Loss: 1.2912\n",
            "Epoch 5, Batch 13500, Loss: 1.3547\n",
            "Epoch 5, Batch 13600, Loss: 1.2557\n",
            "Epoch 5, Batch 13700, Loss: 1.2800\n",
            "Epoch 5, Batch 13800, Loss: 1.2931\n",
            "Epoch 5, Batch 13900, Loss: 1.2821\n",
            "Epoch 5, Batch 14000, Loss: 1.2929\n",
            "Epoch 5, Batch 14100, Loss: 1.2581\n",
            "Epoch 5, Batch 14200, Loss: 1.2738\n",
            "Epoch 5, Batch 14300, Loss: 1.2630\n",
            "Epoch 5, Batch 14400, Loss: 1.3053\n",
            "Epoch 5, Batch 14500, Loss: 1.2845\n",
            "Epoch 5, Batch 14600, Loss: 1.3196\n",
            "Epoch 5, Batch 14700, Loss: 1.2769\n",
            "Epoch 5, Batch 14800, Loss: 1.3241\n",
            "Epoch 5, Batch 14900, Loss: 1.2976\n",
            "Epoch 5, Batch 15000, Loss: 1.3103\n",
            "Epoch 5, Batch 15100, Loss: 1.2818\n",
            "Epoch 5, Batch 15200, Loss: 1.2076\n",
            "Epoch 5, Batch 15300, Loss: 1.2842\n",
            "Epoch 5, Batch 15400, Loss: 1.2912\n",
            "Epoch 5, Batch 15500, Loss: 1.3379\n",
            "Epoch 5, Batch 15600, Loss: 1.2753\n",
            "Epoch 5, Batch 15700, Loss: 1.2848\n",
            "Epoch 5, Batch 15800, Loss: 1.3019\n",
            "Epoch 5, Batch 15900, Loss: 1.2740\n",
            "Epoch 5, Batch 16000, Loss: 1.3189\n",
            "Epoch 5, Batch 16100, Loss: 1.2920\n",
            "Epoch 5, Batch 16200, Loss: 1.3342\n",
            "Epoch 5, Batch 16300, Loss: 1.2774\n",
            "Epoch 5, Batch 16400, Loss: 1.3001\n",
            "Epoch 5, Batch 16500, Loss: 1.2526\n",
            "Epoch 5, Batch 16600, Loss: 1.3024\n",
            "Epoch 5, Batch 16700, Loss: 1.3215\n",
            "Epoch 5, Batch 16800, Loss: 1.2629\n",
            "Epoch 5, Batch 16900, Loss: 1.2724\n",
            "Epoch 5, Batch 17000, Loss: 1.2820\n",
            "Epoch 5, Batch 17100, Loss: 1.2705\n",
            "Epoch 5, Batch 17200, Loss: 1.2463\n",
            "Epoch 5, Batch 17300, Loss: 1.2813\n",
            "Epoch 5, Batch 17400, Loss: 1.2303\n",
            "Epoch 5 completed, Average Loss: 1.2910\n",
            "\n",
            "Generating text...\n",
            "\n",
            "Generated text:\n",
            "To be or not to be, saw,\n",
            "For being were fellow is here.\n",
            "\n",
            "MENENIUS:\n",
            "Plantageness to their royal scarce the other friends, in good news.\n",
            "\n",
            "WARWICK:\n",
            "Then, come with stocks, with lord?\n",
            "\n",
            "POMPEY:\n",
            "By my faces,\n",
            "To have the fiel\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}