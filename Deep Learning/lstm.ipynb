{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter\n",
        "import random"
      ],
      "metadata": {
        "id": "uOnt94pO-0jY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Step 1: Create a Simple Dataset Class\n",
        "# ====================================\n",
        "class MovieReviewDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom Dataset class for handling movie reviews.\n",
        "    This helps PyTorch efficiently load and batch our data.\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, vocab, max_length=100):\n",
        "        self.texts = texts          # List of review texts\n",
        "        self.labels = labels        # List of labels (0 for negative, 1 for positive)\n",
        "        self.vocab = vocab          # Dictionary mapping words to unique indices\n",
        "        self.max_length = max_length # Maximum length of each review\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of reviews\"\"\"\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Converts a text review into a sequence of numbers and returns it with its label\n",
        "        idx: Index of the review to fetch\n",
        "        \"\"\"\n",
        "        # Get the review text and its label\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert text to list of tokens (words)\n",
        "        tokens = self.tokenize(text)\n",
        "\n",
        "        # Convert tokens to numerical indices using vocabulary\n",
        "        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
        "\n",
        "        # Pad or truncate to max_length\n",
        "        if len(indices) < self.max_length:\n",
        "            indices = indices + [self.vocab['<pad>']] * (self.max_length - len(indices))\n",
        "        else:\n",
        "            indices = indices[:self.max_length]\n",
        "\n",
        "        return torch.tensor(indices), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        \"\"\"\n",
        "        Converts text to lowercase and splits it into words\n",
        "        Removes punctuation for simplicity\n",
        "        \"\"\"\n",
        "        text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "        return text.split()"
      ],
      "metadata": {
        "id": "TQjyiwBa-2k5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Step 2: Define the LSTM Model\n",
        "# ====================================\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, n_layers=2, dropout=0.3):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        # Word embedding layer with uniform initialization\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
        "\n",
        "        # Batch normalization for embeddings\n",
        "        self.embed_bn = nn.BatchNorm1d(embedding_dim)\n",
        "\n",
        "        # Bidirectional LSTM\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_dim,\n",
        "            n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if n_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Additional linear layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Initialize weights for linear layers only\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        \"\"\"Initialize weights for linear layers only\"\"\"\n",
        "        for layer in [self.fc1, self.fc2]:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Embed and normalize\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "        # Reshape for batch norm\n",
        "        batch_size, seq_len, embed_dim = embedded.shape\n",
        "        embedded = embedded.view(-1, embed_dim, seq_len)\n",
        "        embedded = self.embed_bn(embedded)\n",
        "        embedded = embedded.view(batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "\n",
        "        # Get final output using both directions\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "        # Dense layers with activations\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.sigmoid(self.fc2(out))\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "db3g1gdd-5T2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Step 3: Training Function\n",
        "# ====================================\n",
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Performs one epoch of training\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch_idx, (text, labels) in enumerate(train_loader):\n",
        "        # Move data to device (CPU/GPU)\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "        # Clear previous gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(text)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(predictions, labels.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate statistics\n",
        "        total_loss += loss.item()\n",
        "        rounded_preds = (predictions > 0.5).float()\n",
        "        correct_predictions += (rounded_preds == labels.unsqueeze(1)).sum().item()\n",
        "        total_predictions += labels.size(0)\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Batch {batch_idx}: Loss = {loss.item():.4f}')\n",
        "\n",
        "    # Calculate epoch statistics\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model on given data\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for text, labels in data_loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, labels.unsqueeze(1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            rounded_preds = (predictions > 0.5).float()\n",
        "            correct_predictions += (rounded_preds == labels.unsqueeze(1)).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "\n",
        "    return total_loss / len(data_loader), correct_predictions / total_predictions"
      ],
      "metadata": {
        "id": "CRENgIEF-7hU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# Step 4: Example Usage\n",
        "# ====================================\n",
        "def predict_sentiment(model, text, vocab, max_length=100, device='cpu'):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a single text review\n",
        "    Returns: probability of positive sentiment and predicted class\n",
        "    \"\"\"\n",
        "    # Prepare model for evaluation\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and convert to indices\n",
        "    tokens = MovieReviewDataset.tokenize(text)\n",
        "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "    # Pad sequence\n",
        "    if len(indices) < max_length:\n",
        "        indices = indices + [vocab['<pad>']] * (max_length - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_length]\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    with torch.no_grad():\n",
        "        input_tensor = torch.tensor([indices]).to(device)\n",
        "        prediction = model(input_tensor)\n",
        "        probability = prediction.item()\n",
        "        predicted_class = 1 if probability >= 0.5 else 0\n",
        "\n",
        "    return probability, predicted_class\n",
        "\n",
        "def test_model_predictions(model, vocab, device):\n",
        "    \"\"\"Test the model with some sample reviews\"\"\"\n",
        "    test_reviews = [\n",
        "        \"This movie was absolutely fantastic and I loved every minute of it!\",\n",
        "        \"What a terrible waste of time, one of the worst movies ever.\",\n",
        "        \"The acting was good but the plot was a bit confusing.\",\n",
        "        \"A masterpiece of modern cinema, brilliant in every way.\",\n",
        "        \"I fell asleep during this boring and predictable film.\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nTesting model predictions:\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for review in test_reviews:\n",
        "        prob, pred = predict_sentiment(model, review, vocab, device=device)\n",
        "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "        confidence = prob if pred == 1 else (1 - prob)\n",
        "\n",
        "        print(f\"\\nReview: {review}\")\n",
        "        print(f\"Sentiment: {sentiment} (confidence: {confidence:.2%})\")\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Expanded sample data\n",
        "    sample_reviews = [\n",
        "        \"this movie was amazing and I loved it\",\n",
        "        \"terrible waste of time, awful movie\",\n",
        "        \"great acting and wonderful story\",\n",
        "        \"boring and predictable plot\",\n",
        "        \"fantastic performances by all actors\",\n",
        "        \"waste of money and time\",\n",
        "        \"incredible cinematography and effects\",\n",
        "        \"disappointing and confusing plot\",\n",
        "        \"brilliant direction and storytelling\",\n",
        "        \"poorly written and badly acted\",\n",
        "        \"masterpiece of modern cinema\",\n",
        "        \"complete disaster of a film\",\n",
        "        \"outstanding performance by the cast\",\n",
        "        \"worst movie I've ever seen\",\n",
        "        \"beautiful cinematography and score\",\n",
        "        \"awful dialogue and weak plot\"\n",
        "    ] * 2  # Duplicate data for more training samples\n",
        "\n",
        "    sample_labels = [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0] * 2\n",
        "\n",
        "    # Split data with larger training set\n",
        "    split_idx = int(len(sample_reviews) * 0.8)\n",
        "    train_reviews = sample_reviews[:split_idx]\n",
        "    train_labels = sample_labels[:split_idx]\n",
        "    val_reviews = sample_reviews[split_idx:]\n",
        "    val_labels = sample_labels[split_idx:]\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocab = {'<pad>': 0, '<unk>': 1}\n",
        "    words = set(' '.join(sample_reviews).lower().split())\n",
        "    for idx, word in enumerate(words, start=len(vocab)):\n",
        "        vocab[word] = idx\n",
        "\n",
        "    # Create datasets and dataloaders with smaller batch size\n",
        "    train_dataset = MovieReviewDataset(train_reviews, train_labels, vocab)\n",
        "    val_dataset = MovieReviewDataset(val_reviews, val_labels, vocab)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "    # Model initialization with smaller dimensions\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = SentimentLSTM(\n",
        "        vocab_size=len(vocab),\n",
        "        embedding_dim=32,  # Reduced from 64\n",
        "        hidden_dim=32,    # Reduced from 64\n",
        "        n_layers=1,       # Reduced from 2\n",
        "        dropout=0.2       # Reduced from 0.3\n",
        "    ).to(device)\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.005, weight_decay=0.001)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.7, patience=3, verbose=True\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 20\n",
        "    best_val_accuracy = 0\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "        if val_acc > best_val_accuracy:\n",
        "            best_val_accuracy = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "    print(f\"Best validation accuracy: {best_val_accuracy:.2%}\")\n",
        "\n",
        "    # After training, test the model\n",
        "    print(\"\\nTesting the trained model:\")\n",
        "    test_model_predictions(model, vocab, device)\n",
        "\n",
        "    # Save the model and vocabulary for later use\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab,\n",
        "        'model_config': {\n",
        "            'vocab_size': len(vocab),\n",
        "            'embedding_dim': 32,\n",
        "            'hidden_dim': 32,\n",
        "            'n_layers': 1,\n",
        "            'dropout': 0.2\n",
        "        }\n",
        "    }, 'sentiment_model.pth')\n",
        "\n",
        "# Add standalone prediction script\n",
        "if __name__ == \"__main__\":\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Using GPU\")\n",
        "    else:\n",
        "        print(\"Using CPU\")\n",
        "\n",
        "    main()\n",
        "\n",
        "# Example of loading and using the saved model:\n",
        "def load_and_predict(text, model_path='sentiment_model.pth'):\n",
        "    \"\"\"Load the saved model and make a prediction\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load the saved model and vocabulary\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    config = checkpoint['model_config']\n",
        "\n",
        "    # Initialize model with saved configuration\n",
        "    model = SentimentLSTM(**config).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Make prediction\n",
        "    prob, pred = predict_sentiment(model, text, vocab, device=device)\n",
        "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "    confidence = prob if pred == 1 else (1 - prob)\n",
        "\n",
        "    return sentiment, confidence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJJU0FId-wfV",
        "outputId": "73e76eb4-6ef7-4d61-a0c2-4b6e5bc91ca9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "Starting training...\n",
            "Batch 0: Loss = 0.6641\n",
            "\n",
            "Epoch 1/20\n",
            "Train Loss: 0.7961 | Train Acc: 48.00%\n",
            "Val Loss: 0.6928 | Val Acc: 57.14%\n",
            "Batch 0: Loss = 0.6762\n",
            "\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6534 | Train Acc: 56.00%\n",
            "Val Loss: 0.7147 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.8833\n",
            "\n",
            "Epoch 3/20\n",
            "Train Loss: 0.7376 | Train Acc: 52.00%\n",
            "Val Loss: 0.7024 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7421\n",
            "\n",
            "Epoch 4/20\n",
            "Train Loss: 0.7335 | Train Acc: 44.00%\n",
            "Val Loss: 0.7017 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7045\n",
            "\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6789 | Train Acc: 64.00%\n",
            "Val Loss: 0.6908 | Val Acc: 57.14%\n",
            "Batch 0: Loss = 0.6321\n",
            "\n",
            "Epoch 6/20\n",
            "Train Loss: 0.7038 | Train Acc: 60.00%\n",
            "Val Loss: 0.6882 | Val Acc: 57.14%\n",
            "Batch 0: Loss = 0.7695\n",
            "\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6919 | Train Acc: 52.00%\n",
            "Val Loss: 0.6852 | Val Acc: 57.14%\n",
            "Batch 0: Loss = 0.6616\n",
            "\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6777 | Train Acc: 72.00%\n",
            "Val Loss: 0.6885 | Val Acc: 57.14%\n",
            "Batch 0: Loss = 0.7265\n",
            "\n",
            "Epoch 9/20\n",
            "Train Loss: 0.7036 | Train Acc: 44.00%\n",
            "Val Loss: 0.7101 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.6354\n",
            "\n",
            "Epoch 10/20\n",
            "Train Loss: 0.7160 | Train Acc: 52.00%\n",
            "Val Loss: 0.7153 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7458\n",
            "\n",
            "Epoch 11/20\n",
            "Train Loss: 0.7099 | Train Acc: 44.00%\n",
            "Val Loss: 0.7128 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.6940\n",
            "\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6902 | Train Acc: 40.00%\n",
            "Val Loss: 0.6980 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7247\n",
            "\n",
            "Epoch 13/20\n",
            "Train Loss: 0.7111 | Train Acc: 40.00%\n",
            "Val Loss: 0.6961 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.6101\n",
            "\n",
            "Epoch 14/20\n",
            "Train Loss: 0.7062 | Train Acc: 44.00%\n",
            "Val Loss: 0.6939 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7269\n",
            "\n",
            "Epoch 15/20\n",
            "Train Loss: 0.7047 | Train Acc: 48.00%\n",
            "Val Loss: 0.6983 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7148\n",
            "\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6859 | Train Acc: 56.00%\n",
            "Val Loss: 0.7033 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.6389\n",
            "\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6867 | Train Acc: 60.00%\n",
            "Val Loss: 0.7138 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.7509\n",
            "\n",
            "Epoch 18/20\n",
            "Train Loss: 0.7398 | Train Acc: 52.00%\n",
            "Val Loss: 0.7162 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.8254\n",
            "\n",
            "Epoch 19/20\n",
            "Train Loss: 0.6968 | Train Acc: 48.00%\n",
            "Val Loss: 0.7068 | Val Acc: 42.86%\n",
            "Batch 0: Loss = 0.6794\n",
            "\n",
            "Epoch 20/20\n",
            "Train Loss: 0.6995 | Train Acc: 48.00%\n",
            "Val Loss: 0.7077 | Val Acc: 42.86%\n",
            "\n",
            "Training completed!\n",
            "Best validation accuracy: 57.14%\n",
            "\n",
            "Testing the trained model:\n",
            "\n",
            "Testing model predictions:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Review: This movie was absolutely fantastic and I loved every minute of it!\n",
            "Sentiment: Positive (confidence: 53.59%)\n",
            "\n",
            "Review: What a terrible waste of time, one of the worst movies ever.\n",
            "Sentiment: Positive (confidence: 53.59%)\n",
            "\n",
            "Review: The acting was good but the plot was a bit confusing.\n",
            "Sentiment: Positive (confidence: 53.59%)\n",
            "\n",
            "Review: A masterpiece of modern cinema, brilliant in every way.\n",
            "Sentiment: Positive (confidence: 53.59%)\n",
            "\n",
            "Review: I fell asleep during this boring and predictable film.\n",
            "Sentiment: Positive (confidence: 53.59%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}